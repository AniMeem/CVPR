{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[0;32m     41\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, img)\n\u001b[1;32m---> 42\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     image \u001b[38;5;241m=\u001b[39m img_to_array(image)\n\u001b[0;32m     44\u001b[0m     image \u001b[38;5;241m=\u001b[39m preprocess_input(image)\n",
      "File \u001b[1;32mI:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:313\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.utils.load_img\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    278\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.preprocessing.image.load_img\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_img\u001b[39m(path, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m              interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    281\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads an image into PIL format.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m  Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m      ValueError: if interpolation method is not supported.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrayscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mI:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:111\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    109\u001b[0m     color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not import PIL.Image. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "#neccesary library import\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "#initializing learning rate, number of epochs and batch size\n",
    "Learn_rate = 1e-4\n",
    "Epoch      = 20\n",
    "Batch_size = 32\n",
    "\n",
    "#dataset path\n",
    "Directory = 'J:/11th Semester/CVPR/FINAL/Assignment-2/Data'\n",
    "Categories = [\"with_mask\", \"without_mask\",\"mask_weared_incorrect\"]\n",
    "\n",
    "data   = []\n",
    "labels = []\n",
    "\n",
    "for category in Categories:\n",
    "    path = os.path.join(Directory, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size=(224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    " \n",
    "    data.append(image)\n",
    "    labels.append(category)\n",
    "\n",
    "#converting aplhabetical values of labels into number values\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "#converting data and labels into numpy array\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "#spliting train data and test data\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=16)\n",
    "#trainX = train image, trainY = train image labels; testX = test image, testY = test image label\n",
    "#test_size = 0.20; 20% images set for testing\n",
    "\n",
    "#data augmentation generating multiple variations of each image wih following properties\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range      = 20,              \n",
    "    zoom_range          = 0.15,\n",
    "    width_shift_range   = 0.2,\n",
    "    height_shift_range  = 0.2,\n",
    "    shear_range         = 0.15,           #the angle of tilt \n",
    "    horizontal_flip     = True,\n",
    "    fill_mode           = \"nearest\"       #replaces empty areas with nearest pixel values\n",
    "    )    \n",
    "\n",
    "#loading MobileNetV2 network\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "#previously trained imagenet is used on weight; initital fully connected layer is left off; shape of the image is defined (224 × 224 ×3) \n",
    "\n",
    "#constructing head of the model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name = \"flatten\")(headModel)\n",
    "headModel = Dense(128, activation = \"relu\")(headModel)        #dense layer with 128 neurons; relu for non-linear use case\n",
    "headModel = Dropout(0.5)(headModel)                           #Just to avoid any overfitting\n",
    "headModel = Dense(2, activation = \"softmax\")(headModel)       #output has 2 layers; dealing with binary classifications and so used softmax\n",
    "\n",
    "#calling the model function that accepts two parameters\n",
    "#Head of the model is placed on top of base model\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "\n",
    "#freezing all of the layers of base model so they are not going to be updated during first training process\n",
    "for layer in baseModel.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "#compiling the model\n",
    "opt = tf.keras.optimizers.legacy.Adam(lr = Learn_rate, decay = Learn_rate/Epoch)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "#training the head of the network\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size = Batch_size),\n",
    "    steps_per_epoch = len(trainX) // Batch_size,\n",
    "    validation_data = (testX, testY),\n",
    "    validation_steps = len(testX) // Batch_size,\n",
    "    epochs = Epoch\n",
    ")\n",
    "\n",
    "#making predictions on the testing set\n",
    "predict_m = model.predict(testX, batch_size = Batch_size)\n",
    "\n",
    "#finding the index of the label with corresponding largest predicted probability\n",
    "predict_m = np.argmax(predict_m, axis = 1)\n",
    "\n",
    "#formatting the classification report\n",
    "print(classification_report(testY.argmax(axis = 1), predict_m, target_names = lb.classes_))\n",
    "\n",
    "#saving the model\n",
    "model.save('J:/11th Semester/CVPR/FINAL/Assignment-2/SavedFiles/model.hdf5')\n",
    "\n",
    "#plotting training loss and accuracy\n",
    "N = Epoch\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label = \"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label = \"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label = \"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label = \"val_acc\")\n",
    "plt.title(\"Training Loss & Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc = \"center right\")\n",
    "\n",
    "#saving the graph as .PNG\n",
    "plt.savefig('J:/11th Semester/CVPR/FINAL/Assignment-2/SavedFiles/graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
